import os, torch
from torch import nn
from util_functions.base import get_config_from_file
from util_functions.prob import fit_gaussian, gaussian_KL, reparameterised_draw, mean_NLL


__all__ = [
    'VAEEnsembleReplicationLossBase',
    'VAEEnsembleKLReplicationLoss',
    'VAEEnsembleReverseKLReplicationLoss',
    'VAEEnsembleNLLReplicationLoss',
    'VAEEnsembleReverseNLLReplicationLoss'
]


class VAEEnsembleReplicationLossBase(nn.Module):
    def __init__(self, config_dir):
        
        # Avoid circular import
        from classes_utils.architecture_integration import AudioEncoderDecoderEnsemble

        super(VAEEnsembleReplicationLossBase, self).__init__()
        config_file = os.join(config_dir, "config.json")
        try:
            kwargs = get_config_from_file(config_file['__target_autoencoder_kwargs'])
        except KeyError:
            raise ValueError(f'Config directory {config_file} not a valid autoencoder ensemble config file (no __target_autoencoder_kwargs)')
        self.autoencoder_ensemble = AudioEncoderDecoderEnsemble(**kwargs)
        raise Exception('load weights here')
        self.autoencoder_ensemble.eval()
        # assert not self.autoencoder_ensemble.variational
        self.z_dim = self.autoencoder_ensemble.encoder_ensemble.embedding_dim

    def generate_targets(self, x):
        with torch.no_grad():
            encodings = self.autoencoder_ensemble.encoder_ensemble(x)
            sample_means = torch.mean(torch.stack(encodings), dim=0)
            sample_covariances = fit_gaussian(encodings, sample_means)
        return sample_means, sample_covariances
    
    def forward(self, encodings, zs, batch):
        means = [e[:,:self.z_dim] for e in encodings]
        log_vars = [e[:,self.z_dim:] for e in encodings]
        features = batch['padded_features']
        return self.forward_method(means, log_vars, features)


class VAEEnsembleKLReplicationLoss(VAEEnsembleReplicationLossBase):
    def __init__(self, config_dir):
        raise Exception('do checks')
        super(VAEEnsembleKLReplicationLoss, self).__init__(config_dir)
    
    def generate_targets(self, x):
        # embedding_samples = [(batch_size, embedding_dim)_1, ..., (batch_size, embedding_dim)_M]
        embedding_samples, sample_means = super(VAEEnsembleKLReplicationLoss, self).generate_targets(x)
        return fit_gaussian(embedding_samples, sample_means)

    def prepare_forward(self, means, log_vars, features):
        target_means, target_covariances = self.generate_targets(features)
        stds = [torch.exp(log_var * 0.5) for log_var in log_vars]
        stds = torch.stack([torch.diag_embed(std) for std in stds], dim = 0)
        means_diffs = torch.stack(target_means, dim=0) - means
        return target_covariances, stds, means_diffs

    def forward_method(self, means, log_vars, features):
        # KL(vae || targets)
        target_covariances, stds, means_diffs = self.prepare_forward(means, log_vars, features)
        return gaussian_KL(stds, target_covariances, means_diffs).sum()


class VAEEnsembleReverseKLReplicationLoss(VAEEnsembleKLReplicationLoss):
    def __init__(self, config_dir):
        super(VAEEnsembleReverseKLReplicationLoss, self).__init__(config_dir)

    def forward_method(self, means, log_vars, features):
        # KL(targets || vae)
        target_covariances, stds, means_diffs = self.prepare_forward(means, log_vars, features)
        return gaussian_KL(target_covariances, stds, means_diffs).sum()


class VAEEnsembleNLLReplicationLoss(VAEEnsembleReplicationLossBase):
    def __init__(self, config_dir):
        super(VAEEnsembleNLLReplicationLoss, self).__init__(config_dir)

    def forward_method(self, means, log_vars, features):
        # Get NLL of autoencoder samples wrt set of distributions generated by VAE
        stds = [torch.exp(log_var * 0.5) for log_var in log_vars]
        stds = [torch.diag_embed(std) for std in stds]
        embedding_samples, sample_means = self.generate_targets(features)
        qs = [
            torch.distributions.MultivariateNormal(mu, stds[i]) for i, mu in enumerate(means)
        ]
        nlls = [mean_NLL(q, embedding_samples) for q in qs]
        return sum(nlls)/len(nlls)


class VAEEnsembleReverseNLLReplicationLoss(VAEEnsembleReplicationLossBase):

    def __init__(self, config_dir, draws_per_gaussian=3):
        raise Exception("Not usable")
        super(VAEEnsembleNLLReplicationLoss, self).__init__(config_dir)
        self.draws_per_gaussian = draws_per_gaussian
    
    def draw_from_vae(self, mean, log_var):
        draws = [reparameterised_draw(mean, log_var) for _ in range(self.draws_per_gaussian)]
        return torch.stack(draws, dim=0)

    def forward_method(self, means, log_vars, features):

        # Get NLL of samples from VAE wrt distribution fitted to autoencoder samples
        stds = [torch.exp(log_var * 0.5) for log_var in log_vars]
        stds = [torch.diag_embed(std) for std in stds]
        embedding_samples, sample_means = self.generate_targets(features)
        target_covariance = fit_gaussian(embedding_samples, sample_means)
        qs = [
            torch.distributions.MultivariateNormal(mu, target_covariance[i]) for i, mu in enumerate(sample_means)
        ]
        samples = [self.draw_from_vae(mu, log_vars[i]) for mu, i in enumerate(means)]
        nlls = [mean_NLL(q, samples[i]) for q, i in enumerate(qs)]
        return sum(nlls)/sum(nlls)
