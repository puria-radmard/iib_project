import torch, os, json, pickle
from torch.utils.data import DataLoader
from config import device
from util_functions.data import coll_fn_utt, generate_data_dict_utt, add_certainties_to_data_dict
from util_functions.visualisation.speech_bc_analysis import *
from torch import nn
from classes_utils.audio.data import LabelledClassificationAudioUtteranceDataset
from classes_utils.base.data import ClassificationDAFDataloader
from training_scripts.audio_regression_scripts import audio_regression_script
from config.ootb import las_reg
from util_functions.data import (
    add_certainties_to_data_dict, train_test_split_data_dict,generate_data_dict_utt, 
    combine_data_dicts, data_dict_length_split, split_data_dict_by_labelled
)
import argparse
from util_functions.base import config_savedir

WORD_ALIGNMENT_PATH='/home/alta/BLTSpeaking/active_learning-pr450/models/baseline/CTDF1_b50/tdnn-f/decode_LM1-int_b50.unlabelled_b50/score_mbr_10/unlabelled_b50.utt.ctm'


parser = argparse.ArgumentParser()
parser.add_argument("--no_ami", action='store_true')
parser.add_argument("--architecture_name", required=True, type=str)
parser.add_argument("--lr", required=True, type=float, help="Learning rate of RAE optimizer")
parser.add_argument("--scheduler_epochs", required=True, nargs="+", type=int)
parser.add_argument("--scheduler_proportion", required=True, type=float)
parser.add_argument("--dropout", required=True, type=float)
parser.add_argument("--weight_decay",required=True,type=float)
parser.add_argument("--batch_size", required=True, type=int)
parser.add_argument("--num_epochs", required=True, type=int)
parser.add_argument("--features_paths",required=True,nargs="+",help="List of paths where .ark files are")
parser.add_argument("--labelled_list",required=True,type=str,help="Path to list of labelled utts")
parser.add_argument("--unlabelled_list",required=True,type=str,help="Path to list of unlabelled utts")
parser.add_argument("--max_seq_len",required=True,type=int)
parser.add_argument("--test_prop",required=True,type=float)
parser.add_argument("--save_dir", required=False, default=None)


def full_analysis_pipeline(config_dir, acquisition_modes):

    # We'll save key information generated by this script here, which we can reuse later
    analysis_recreation_dict = {}

    # Get evaluations, configurations, and results
    eval_dict = torch.load(os.path.join(config_dir, 'evaluations.pkl'))
    conf_dict = get_json(os.path.join(config_dir, 'config.json'))
    resl_dict_list = get_json(os.path.join(config_dir, 'results.json'))

    # Split CDFs
    split_cdf_fig, split_cdf_axes = plt.subplots(1, figsize=(12, 8))
    analysis_recreation_dict['labelled_score_cdf'] = cdf_plot(filter_ami_from_labelled_eval(eval_dict)[:,1], 50, split_cdf_axes, label='Labelled BULATS')
    analysis_recreation_dict['unlabelled_score_cdf'] = cdf_plot(eval_dict['unlabelled_preds'][:,1], 50, split_cdf_axes, label='Unlabelled BULATS')

    # Learning curve
    train_loss_fig, train_loss_axes = plt.subplots(1, figsize=(12, 8))
    train_loss_axes.plot([r['train loss'] for r in resl_dict_list])
    train_loss_axes.set_xlabel('Epoch')
    train_loss_axes.set_ylabel('Training Loss')
    analysis_recreation_dict['training_curve'] = [r['train loss'] for r in resl_dict_list]

    # Scatter plot against confidence
    confidence_scatter_fig, confidence_scatter_axes = plt.subplots(1, figsize=(12, 8))
    confidence_scatter_axes.set_ylim(0, 1)
    confidence_scatter_axes.set_title(f'LR = {str(conf_dict["lr"])}')
    analysis_recreation_dict['score_vs_certainty_scatter'] = scatter_daf_scores_against_lc_asr(eval_dict, confidence_scatter_axes, 'BC p(labelled)')


    # Value and value div time acquisition band proportions
    props_hist_fig, props_hist_axs = plt.subplots(
        len(acquisition_modes) + 1, 1, figsize=(12, 8 * (len(acquisition_modes) + 1))
    )

    # Will go directly to to analysis_recreation_dict shortly
    agents_dict, models_dict, props_dict, durs_dict, utts_dict = {}, {}, {}, {}, {}

    # Start with acquisition/proportions for confidence based
    agents_dict['conf'], models_dict['conf'] = get_acq_set('conf', conf_dict, eval_dict, WORD_ALIGNMENT_PATH)
    props_dict['conf'], durs_dict['conf'], utts_dict['conf'] = acquisition_proportions_history(make_acquisition(agents_dict['conf']), models_dict['conf'], True)
    make_proportions_plot(durs_dict['conf'], props_dict['conf'], props_hist_axs[0])
    props_hist_axs[0].set_title('conf props')

    # Iterate through requested modes' acquisition/proportions
    for i, acq_mode in enumerate(acquisition_modes, 1):
        agents_dict[acq_mode], models_dict[acq_mode] = get_acq_set(acq_mode, conf_dict, eval_dict, WORD_ALIGNMENT_PATH)
        props_dict[acq_mode], durs_dict[acq_mode], utts_dict[acq_mode] = acquisition_proportions_history(
            make_acquisition(agents_dict[acq_mode]), models_dict[acq_mode], True
        )
        make_proportions_plot(durs_dict[acq_mode], props_dict[acq_mode], props_hist_axs[i])
        props_hist_axs[i].set_title(f'{acq_mode} props')

    analysis_recreation_dict['proportion_histories'] = props_dict
    analysis_recreation_dict['acquisiton_durations'] = durs_dict
    analysis_recreation_dict['acquisition_utterances'] = utts_dict


    # Confidences CDF
    confidence_cdfs_fig, confidence_cdfs_axs = plt.subplots(1, figsize=(12, 8))
    analysis_recreation_dict['confidence_cdfs_dict'] = confidence_cdf_from_agents(
        models_dict['conf'], confidence_cdfs_axs, **agents_dict
    )

    # Avg confidence against acquisition
    avg_conf_against_acq_fig, avg_conf_against_acq_axes = plt.subplots(1, figsize=(12, 8))
    analysis_recreation_dict['running_average_dict'] = plot_cumulative_average_confidence(
        avg_conf_against_acq_axes, **agents_dict
    )

    # Save all our images
    split_cdf_fig.savefig(os.path.join(config_dir, 'split_cdf.png'))
    train_loss_fig.savefig(os.path.join(config_dir, 'train_loss.png'))
    confidence_scatter_fig.savefig(os.path.join(config_dir, 'confidence_scatter.png'))
    props_hist_fig.savefig(os.path.join(config_dir, 'props_hist.png'))
    confidence_cdfs_fig.savefig(os.path.join(config_dir, 'confidence_cdfs.png'))
    avg_conf_against_acq_fig.savefig(os.path.join(config_dir, 'avg_conf_against_acq.png'))

    # Save the plots numerically
    with open(os.path.join(config_dir, 'analysis_recreation_dict.json'), 'wb') as f:
        pickle.dump(analysis_recreation_dict, f)

    print('Analysis figures done for', config_dir, '!!')


def run_evaluations(evaluation_model, evalutation_dataloader, get_certainties):
    all_preds = torch.empty(0, 2)
    all_embeddings = torch.empty(0, 256)
    all_utt_ids = []
    all_certainties = torch.empty(0)

    with torch.no_grad():

        for i, batch in enumerate(evalutation_dataloader):

            if i > 0 and i%10 == 0:
                print(f'Batch {i} | {len(evalutation_dataloader)} done')

            embeddings, preds = evaluation_model(batch['padded_features'].to(device))
            
            all_preds = torch.cat([all_preds, preds[0].detach().cpu()])
            all_embeddings = torch.cat([all_embeddings, embeddings[0].detach().cpu()])
            all_utt_ids.extend(batch['utt_id'])
            if get_certainties:
                all_certainties = torch.cat([all_certainties, batch['certainties'].detach().cpu()])
    
    if get_certainties:
        return all_preds, all_embeddings, all_utt_ids, all_certainties
    return all_preds, all_embeddings, all_utt_ids


def evaluations_pipeline(
    model, config_path_base, original_data_dict, ctm_path, max_seq_len, 
    unlabelled_list_path, labelled_list_path, batch_size
    ):
    # e.g.
    # config_path_base = ".../listen_and_attend_classification/config-1"

    save_path = os.path.join(config_path_base, 'evaluations.pkl')

    save_dict = {}
    data_dict = add_certainties_to_data_dict(original_data_dict, [ctm_path])

    # We had to recreate the dataset using the new datadict, which will have confidences
    unlabelled_dataset, _ = labelled_classification_test_train_datasets(
        data_dict=data_dict,
        max_sequence_length=max_seq_len,
        labelled_list_path=None,
        unlabelled_list_path=unlabelled_list_path,
        test_prop=0
    )
    assert len(unlabelled_dataset.indices) == 0
    unlabelled_dataloader = DataLoader(unlabelled_dataset, shuffle = False, collate_fn=coll_fn_utt, batch_size=batch_size)

    print("starting unlabelled")
    unlabelled_preds, unlabelled_embeddings, unlabelled_utt_ids, unlabelled_certainties = \
        run_evaluations(model, unlabelled_dataloader, True)
    save_dict["unlabelled_preds"] = unlabelled_preds
    save_dict["unlabelled_embeddings"] = unlabelled_embeddings
    save_dict["unlabelled_utt_ids"] = unlabelled_utt_ids
    save_dict["unlabelled_certainties"] = unlabelled_certainties

    # Now use the original one, which still has the labelled utterance sin
    labelled_dataset, _ = labelled_classification_test_train_datasets(
        data_dict=original_data_dict,
        max_sequence_length=max_seq_len,
        labelled_list_path=labelled_list_path,
        unlabelled_list_path=None,
        test_prop=0
    )
    labelled_dataloader = DataLoader(labelled_dataset, shuffle = False, collate_fn=coll_fn_utt, batch_size=batch_size)
    print("starting labelled")
    labelled_preds, labelled_embeddings, labelled_utt_ids = \
        run_evaluations(model, labelled_dataloader, False)
    save_dict["labelled_preds"] = labelled_preds
    save_dict["labelled_embeddings"] = labelled_embeddings
    save_dict["labelled_utt_ids"] = labelled_utt_ids

    torch.save(save_dict, save_path)
    print('Saved in', save_path)


def labelled_classification_test_train_datasets(
        data_dict, max_sequence_length, labelled_list_path, unlabelled_list_path, test_prop
    ):
    # Get the data dicts and split by labelled and unlabelled 
    data_dict = data_dict_length_split(data_dict, max_sequence_length)
    labelled_data_dict, unlabelled_data_dict = split_data_dict_by_labelled(
        data_dict, labelled_list_path, unlabelled_list_path
    )

    if test_prop > 0:
        # Split both subsets by train-test
        train_labelled_data_dict, test_labelled_data_dict = train_test_split_data_dict(labelled_data_dict, test_prop)
        train_unlabelled_data_dict, test_unlabelled_data_dict = train_test_split_data_dict(unlabelled_data_dict, test_prop)
        
        # Recombine them
        train_data_dict = combine_data_dicts(train_labelled_data_dict, train_unlabelled_data_dict)
        test_data_dict = combine_data_dicts(test_labelled_data_dict, test_unlabelled_data_dict)

        test_dataset = LabelledClassificationAudioUtteranceDataset(
            audio=test_data_dict['mfcc'],
            utt_ids=test_data_dict['utterance_segment_ids'],
            dim_means="config/per_speaker_mean.pkl",
            dim_stds="config/per_speaker_std.pkl",
            init_labelled_indices=set(range(len(test_labelled_data_dict['mfcc']))),
            **{k: v for k, v in test_data_dict.items() if k not in ['mfcc', 'utterance_segment_ids']}
        )

    else:
        train_labelled_data_dict = labelled_data_dict
        train_data_dict = combine_data_dicts(labelled_data_dict, unlabelled_data_dict)
        test_dataset = None
        
    # Put data dicts into datasets
    train_dataset = LabelledClassificationAudioUtteranceDataset(
        audio=train_data_dict['mfcc'],
        utt_ids=train_data_dict['utterance_segment_ids'],
        dim_means="config/per_speaker_mean.pkl",
        dim_stds="config/per_speaker_std.pkl",
        init_labelled_indices=set(range(len(train_labelled_data_dict['mfcc']))),
        **{k: v for k, v in train_data_dict.items() if k not in ['mfcc', 'utterance_segment_ids']}
    )

    return train_dataset, test_dataset

# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/alta/Users/pr450/anaconda3/envs/bias_investigation/lib
# python
# from interfaces.audio_labelled_classification import full_analysis_pipeline; full_analysis_pipeline('/home/alta/BLTSpeaking/exp-pr450/lent_logs/generalised_listen_and_attend_classification_no_ami/config-4', ['bc', 'tnbc'])

if __name__ == '__main__':

    args = parser.parse_args()
    
    # Make training objects
    model = getattr(las_reg, args.architecture_name)(args.dropout, use_logits=True).to(device)
    opt = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1, gamma=args.scheduler_proportion)
    criterion = nn.CrossEntropyLoss(reduction='mean')

    # Generate data as always
    data_dict = generate_data_dict_utt(args.features_paths, exclude_ami=args.no_ami, text_path=None)
    train_dataset, test_dataset = labelled_classification_test_train_datasets(
        data_dict, args.max_seq_len, args.labelled_list, args.unlabelled_list, args.test_prop
    )

    # Put datasets into dataloaders
    train_dataloader = ClassificationDAFDataloader(train_dataset, batch_size=args.batch_size)
    if args.test_prop > 0:
        test_dataloader = ClassificationDAFDataloader(test_dataset, batch_size=args.batch_size)
    else:
        test_dataloader = []

    save_dir = config_savedir(args.save_dir, args)

    # Train model
    model, results = audio_regression_script(
        ensemble=model,
        optimizer=opt,
        scheduler=scheduler,
        scheduler_epochs=args.scheduler_epochs,
        criterion=criterion,
        train_dataloader=train_dataloader,
        test_dataloader=test_dataloader,
        num_epochs=args.num_epochs,
        target_attribute_name="labelled",
        is_regression=False,
        show_print=True
    )


    # Save model and results
    with open(os.path.join(save_dir, 'results.json'), 'w') as jf:
        json.dump(results, jf)

    torch.save(model.state_dict(), os.path.join(save_dir, 'model.mdl'))


    # Run evaluations straight away
    ctm_path = '/home/alta/BLTSpeaking/active_learning-pr450/models/baseline/CTDF1_b50/tdnn-f/decode_LM1-int_b50.unlabelled_b50/score_mbr_10/unlabelled_b50.utt.ctm'
    original_data_dict = generate_data_dict_utt(args.features_paths, text_path=None)

    evaluations_pipeline(
        model=model, 
        config_path_base=save_dir, 
        original_data_dict=original_data_dict, 
        ctm_path=ctm_path, 
        max_seq_len=args.max_seq_len,
        unlabelled_list_path=args.unlabelled_list,
        labelled_list_path=args.labelled_list, 
        batch_size=args.batch_size,
    )

    # Finally, do the analysis
    full_analysis_pipeline(save_dir, ['bc', 'tnbc'])
