import torch
from math import e
from scipy.special import lambertw as W0
from scipy.stats import spearmanr

def truncated_sqrt_exponential_pdf(x, beta):
    """
        This is a exp^(x^0.5) pdf truncated at [0, 1] and renormalised
        In integral calculator: (beta^2 e ^{-beta*sqrt(x)})/(2(1-e^-beta - beta*e^-beta))
    """
    numerator = (beta**2) * torch.exp(-beta * torch.sqrt(x))
    exp_term = torch.exp(-beta)
    denonimator = 2*(1 - exp_term - beta * exp_term)
    pdf = numerator / denonimator
    return torch.nan_to_num(pdf * (0<=x) * (x<=1))


def delta_beta_step(x, beta):

    """
        This calculates the GA step when finding maximum likelihood beta
        to fit the truncated_sqrt_exponential_pdf to a data vector x
    """
    # Dataset size
    N = len(x)

    # 2N/beta
    first_term = 2*N/beta

    # e^-beta
    neg_exp_term = torch.exp(-beta)

    # 2beta*e^-beta
    inner_numerator = beta*neg_exp_term

    # 2 - 2e^-beta - 2beta*e^-beta
    inner_denominator = 1 - neg_exp_term - beta*neg_exp_term

    # - sum_i(sqrt(x_i)))
    # This one can be made into an argument??
    final_term = - torch.sqrt(x).sum()

    return first_term - N*(inner_numerator / inner_denominator) + final_term


def sample_from_custom_distribution(pdf_function, pdf_args, support_start, support_end, num_bins, num_samples):
    """
        Given a callable pdf_function, a support range on the real number line, and a resolution,
        generate a set of samples from the distribution (up to the resolution)
    """
    # Generate the support, given the number of bins (number of intervals, hence the +1)
    support = torch.linspace(support_start, support_end, num_bins + 1)

    # For each of the bins get the central point. This will help parallelise the pdf_function call
    bin_centers = 0.5 * (support[:-1] + support[1:])

    # Evaluate the pdf function at each point
    pdf_profile = pdf_function(bin_centers, *pdf_args)

    # For each of the bins, get the width. This will help with density calculations
    bin_widths = torch.diff(support)

    # Get the density in each bin, and help verify for the user
    bin_densities = bin_widths * pdf_profile
    print(f"Total density integral for support provided = {bin_densities.sum()}")

    # Convert the bin_densities into the number of samples that should be drawn uniformly from each bin
    bin_populations = (bin_densities * num_samples).to(int)

    # Iterate over bins and add to our samples
    all_samples = torch.tensor([])
    
    for i, bin_num_samples in enumerate(bin_populations):

        # Range of the local uniform distribution
        bin_start, bin_end = support[i], support[i+1]

        # Get the samples from this bin
        bin_samples = (bin_end - bin_start) * torch.rand(bin_num_samples) + bin_start

        # Add to all samples
        all_samples = torch.cat([all_samples, bin_samples])

    return all_samples


def fit_beta(data, beta_0, lr, num_iters, include_llh=False):
    """
        Fit beta to a dataset, using gradient descent
    """

    # Initialise the loglikelihood and beta histroy arrays
    llh_0 = torch.log(truncated_sqrt_exponential_pdf(data, beta_0)).sum() if include_llh else 0
    llhs = [llh_0]
    betas = [beta_0]

    # Initialise beta
    beta = beta_0

    # Begin gradient descent
    for _ in range(num_iters):

        # Get the gradient at this point
        grad = delta_beta_step(data, beta)

        # Update beta
        beta = beta + lr * grad

        if include_llh:
            # Get the llh for the new beta
            lh = truncated_sqrt_exponential_pdf(data, beta)
            llh = torch.log(lh).sum()
            llhs.append(llh)

        # Log beta value
        betas.append(beta)

    transformed_data = jacobian_transform(data, beta, C=0)

    return beta, betas, llhs, transformed_data.min()


def jacobian_transform(data, beta, C = 1):
    """
        Transform data using the Jacobian function with the infered beta value.
        r.v.s generated by truncated_sqrt_exponential_pdf should become uniformly distributed (both on support [0,1])
        The Jacobian is:
            y(x) = C - ((b*(x^0.5) + 1)*e^(b(1-(x^0.5)))/(e^(b) - b - 1))
    """

    sqrt_data = data**0.5
    numerator_exp = torch.exp(beta * (1 - sqrt_data))
    numerator_coef = -(beta * sqrt_data + 1)
    denominator = torch.exp(beta) - beta - 1
    return C + numerator_exp * numerator_coef / denominator


def inverse_jacobian_transform(transformed_data, beta):
    """
        Inverse the transformation:
            x(y) = ((1 + W0((x - 1)(1 - exp(-beta) - beta*exp(-beta))exp(-1))) / (beta))^2

        where W0(.) is the primary branch of the Lambert W function
    """
    W0_arg = (transformed_data - 1) * (1 - torch.exp(-beta) - beta*torch.exp(-beta)) * (e**-1)
    W0_eval = W0(W0_arg, k=0).real
    return ((W0_eval + 1)/ beta)**2
    


if __name__ == '__main__':
    
    import matplotlib.pyplot as plt
    import sys

    # Shared objects between sim and real
    fig, axs = plt.subplots(1, 4, figsize = (20, 5))

    x = torch.linspace(0, 1, 100)

    BETA = torch.tensor(7)

    beta_0 = torch.tensor(9)
    num_steps = 2000
    lr = 0.00001

    if sys.argv[1] == 'w0':
        y = truncated_sqrt_exponential_pdf(x, BETA)
        x_recovered = inverse_jacobian_transform(y, BETA)

        axs[0].plot(x, y)
        axs[1].plot(y, x_recovered)
        axs[2].plot(x, x_recovered)

        plt.savefig('inverse_function.png')

        exit()

    elif sys.argv[1] == 'real':

        from interfaces.cifar_acquisition_regression import generate_acquisition_regression_dataset
        from active_learning.acquisition.logit_acquisition import *

        train_loader, test_loader, test_dataset = generate_acquisition_regression_dataset(
            logit_bank_path='/home/alta/BLTSpeaking/exp-pr450/lent_logs/acquisition_distillation_100/trained_logits-2/evaluations.pkl',
            acquisition_class=LowestConfidenceAcquisition,
            dataset_name='cifar100',
            batch_size=64,
            test_prop=0.8,
            include_trainset=False
        )

        acquisition_indices = train_loader.dataset.dataset.indices
        lcs = torch.tensor([train_loader.dataset.dataset.dataset.targets[i] for i in acquisition_indices])

        data = lcs + 1

    elif sys.argv[1] == 'sim':

        # Generate the simulated data
        data = sample_from_custom_distribution(truncated_sqrt_exponential_pdf, [BETA], 0, 1, 500, 95000)

        # Plot real beta on the beta history plot
        axs[1].plot([0, num_steps], [BETA, BETA], color='black', linestyle='--')

        # Plot ideal curve
        y = truncated_sqrt_exponential_pdf(x, BETA)
        axs[0].plot(x, y, label = 'generated')


    # Infer the maximum likelihood beta
    infered_beta, beta_history, llh_history, _ = fit_beta(data, beta_0, lr, num_steps)

    # Transform the data - should be uniform
    transformed_data = jacobian_transform(data, infered_beta)

    # Plot data and reverted data
    axs[0].hist(data.numpy(), 50, density = True)

    # Plot the infered beta on the hist
    y = truncated_sqrt_exponential_pdf(x, infered_beta)
    axs[0].plot(x, y, label = 'inferred')
    axs[0].plot(x, inverse_jacobian_transform(y, infered_beta))

    axs[0].legend()

    # Plot beta and llh history over GA
    axs[1].plot(beta_history); axs[1].set_title('beta_history')
    axs[2].plot(llh_history); axs[2].set_title('llh_history')

    # Bar chart the transformed data
    axs[3].hist(transformed_data.numpy(), 50)

    import pdb; pdb.set_trace()
    print(spearmanr(data, transformed_data))

    plt.savefig('custom_samples2.png')
